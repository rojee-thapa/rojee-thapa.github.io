---
layout: post
title: "Day 30 - Parkinson's disease"
date: 2025-07-07
author: Roji Thapa
permalink: /day30.html
tags: ['ReLu','Sigmoid','tanh']

what_i_learned: |
   Today, I continued working with the Parkinson’s disease dataset to evaluate how different models perform. I started by checking the model’s performance without balancing the data and without using cross-validation. Then, I balanced the data and applied cross-validation to get more reliable and fair results. After that, I used our customized Extreme Learning Machine (ELM) model on both balanced and imbalanced data, and tested it with three activation functions: sigmoid, ReLU, and tanh. Comparing the results helped me better understand how data preparation and model choices affect the accuracy and overall performance.

  
blockers: |
   One of the main challenges I faced today was using cross-validation after balancing the dataset. Balancing the data with techniques like SMOTE worked fine on its own, but things got tricky once I combined it with cross-validation. For example, I encountered errors because the data splits were not consistent across the folds. Sometimes, the shapes of the training and testing sets didn’t match, or the labels got mixed up. It took some time to figure out what was going wrong, but I was able to fix the issues by carefully checking the code from the start. 
   


reflection: |
   Even though I faced some problems, I feel good about today’s progress. I’m beginning to understand the steps and the research more clearly. Working with real datasets and models is helping me see what needs to be done for our research, and it’s teaching me something new every day. I’m also getting better at organizing my code and understanding what might cause errors. It’s interesting to see how different techniques can change the results and performance of the models. I’m excited to keep learning and to understand the research and process more deeply.
---
